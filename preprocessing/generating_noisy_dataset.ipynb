{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"discharge_notes_all_umls.ipynb","provenance":[{"file_id":"1Cy7DnUl8bgfyC7YTbNnqzKV2tPlhuUxV","timestamp":1588193870061},{"file_id":"1SRHtrk3XVrxaI3G6TwLC9aAbt_uCwBkx","timestamp":1588183696447},{"file_id":"163gVyZEG8a8ZP7X_DauvVYNkd1igiSDs","timestamp":1588176672317},{"file_id":"16kL0CTH3t705n3kOf2AG16PYRgIL3Zj5","timestamp":1588127877290},{"file_id":"1ZJb2Us6XdLTAZbQdOuRTRlihffYRbvHv","timestamp":1588107814167},{"file_id":"1T1w5Qu30Yz3O7qaVPIY93uJDzniGe9kH","timestamp":1588097640648},{"file_id":"1hPu-cs03U5ECLxoeJ2-myECAs1I6ai1Q","timestamp":1587843940353},{"file_id":"1zBrozmF9QKG4bktxn_8ZeFAex32pIhFY","timestamp":1587841913785},{"file_id":"10x_nZvCShodPDlXxgR3SpjiryvIQqyKI","timestamp":1587830971768},{"file_id":"169R7syXqXZy3S_mMXskY5DiLMoYyEgtd","timestamp":1587827459320},{"file_id":"1abxZvN2e21KyBKKvHPLJ6LZlz-MLVawL","timestamp":1587822424431},{"file_id":"17Q5eyfkz3TnyuZEv1ZDZt4OsL2vqp0Y7","timestamp":1587784169217},{"file_id":"1ruRNcHiVdmsWYOWjL_rloGNkoXgmztW1","timestamp":1587767004831},{"file_id":"1m-o30p66wiBVV_G8wqZBI52_97VdVsqm","timestamp":1587749408135},{"file_id":"13rhM1RTqf4m4xhbYt__H2PAt7GUlEghh","timestamp":1582401033968},{"file_id":"1uLgE5YYlcOl8e5TDrLSCNA-rtSU2h2RB","timestamp":1581803494513},{"file_id":"1zeA-EaOZZ3JkVkp9Z3lhMJ1f54O-HNSv","timestamp":1581567277680}],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y6RYN8wCY2jr","colab_type":"text"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"ElpqUAmUvI7w","colab_type":"code","cellView":"both","colab":{}},"source":["from google.colab import auth\n","import pandas as pd\n","import numpy as np\n","from pandas.core.common import flatten\n","\n","import nltk\n","import re\n","import string\n","import itertools\n","import pickle\n","\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","import nltk\n","nltk.download('punkt')\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from spacy.matcher import PhraseMatcher\n","import io\n","from google.colab import files"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIyJAXiowph8","colab_type":"text"},"source":["## Loading Data\n"]},{"cell_type":"code","metadata":{"id":"Nbm57eDfvNVO","colab_type":"code","colab":{}},"source":["auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4j84nlbOwAYH","colab_type":"text"},"source":["#### Mimic Discharge Data"]},{"cell_type":"code","metadata":{"id":"d6s5t1m_wDMj","colab_type":"code","colab":{}},"source":["!gsutil cp gs://hst-956/adult_notes.gz ./"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oiwcD0kJwJkQ","colab_type":"code","colab":{}},"source":["umls_terms = pickle.load(open('.drive/My Drive/MLHC Final Project/umls_summary.pk', 'rb'))\n","discharge_df = pd.read_csv('./drive/My Drive/MLHC Final Project/MIMIC Data - All Discharge Summaries/first_10000_processed_mimic_notes_data.csv') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHC2IX3RgXrZ","colab_type":"code","colab":{}},"source":["# create spacy phrase matcher \n","# to do exact matching on MIMIC discharge notes to UMLS terms\n","def generatePhraseMatcher():\n","  # read in problem terms in pieces to avoid truncating\n","  prob_pattern_200 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/prob_pattern_200.pickle', 'rb'))\n","  prob_pattern_400 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/prob_pattern_400.pickle', 'rb'))\n","  prob_pattern_600 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/prob_pattern_600.pickle', 'rb'))\n","  prob_pattern_800 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/prob_pattern_800.pickle', 'rb'))\n","\n","  # add problem pattern phrases to PhraseMatcher\n","  pm = PhraseMatcher(nlp.vocab, attr = 'LOWER')\n","\n","  pm.add('problem', None, *prob_pattern_200)\n","  pm.add('problem', None, *prob_pattern_400)\n","  pm.add('problem', None, *prob_pattern_600)\n","  pm.add('problem', None, *prob_pattern_800)\n","\n","  # delete objects to free up memory\n","  del prob_pattern_200\n","  del prob_pattern_400\n","  del prob_pattern_600\n","  del prob_pattern_800\n","\n","  # read in treatment terms in pieces to avoid truncating\n","  treat_pattern_250 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/treat_pattern_250.pickle', 'rb'))\n","  treat_pattern_500 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/treat_pattern_500.pickle', 'rb'))\n","  treat_pattern_750 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/treat_pattern_750.pickle', 'rb'))\n","\n","  # add treatment pattern phrases to PhraseMatcher\n","  pm.add('treatment', None, *treat_pattern_250)\n","  pm.add('treatment', None, *treat_pattern_500)\n","  pm.add('treatment', None, *treat_pattern_750)\n","\n","  # delete objects to free up memory\n","  del treat_pattern_250\n","  del treat_pattern_500\n","  del treat_pattern_750\n","\n","  # read in test terms in pieces to avoid truncating\n","  test_pattern_50 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/test_pattern_50.pickle', 'rb'))\n","  test_pattern_100 = pickle.load(open('./drive/My Drive/MLHC Final Project/Pickled Patterns/test_pattern_100.pickle', 'rb'))\n","  # add test pattern phrases to PhraseMatcher\n","  pm.add('test', None, *test_pattern_50)\n","  pm.add('test', None, *test_pattern_100)\n","\n","  # delete objects to free up memory\n","  del test_pattern_50\n","  del test_pattern_100\n","\n","  return(pm)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUWM7wgX9UrG","colab_type":"code","colab":{}},"source":["# Input to this function is a dataframe with an ID column and a column \n","# with column name 'SENTENCE'\n","def apply_matcher(df, phrase_matcher):\n","\n","  # Create columns with (1) phrase_matcher output, (2) corresponding ids\n","  # (3) start and end of span of each matched phrase, (4) total sentence length\n","  df['MATCH'] = df['SENTENCE'].apply(lambda x : phrase_matcher(nlp(x)))\n","  df['MATCH_IDS'] = df['MATCH'].apply(lambda x : [nlp.vocab.strings[y[0]] for y in list(x)])\n","  df['SPANS_START'] = df['MATCH'].apply(lambda x : [y[1] for y in list(x)])\n","  df['SPANS_END'] = df['MATCH'].apply(lambda x : [y[2] for y in list(x)])\n","  df['SENTENCE_LENGTH'] = df['SENTENCE'].str.split().str.len()\n","\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cHzy2ddS0B1","colab_type":"code","colab":{}},"source":["# Input to this function is a dataframe output from \"apply_matcher\" function\n","def apply_labels(df):\n","\n","  indices_to_keep = []\n","  labels = []\n","  index = 0\n","  \n","  # Find all indices of words that were matched in the sentence\n","  for start in list(range(len(df['SPANS_START']))):\n","    item = list(range(df['SPANS_START'][start],df['SPANS_END'][start]))\n","    indices_to_keep.append(item)\n","  df['ALL_INDICES'] = indices_to_keep\n","\n","  # If no match is found for a word, assign 'O' as the label\n","  for word in df['SENTENCE'].split():\n","    if next((i for i, val in enumerate(df['ALL_INDICES']) if index in val),None) == None:\n","      labels.append('O')\n","    else:\n","      match_id_index = next(i for i, val in enumerate(df['ALL_INDICES']) if index in val)\n","      # If match is found for a word, and its index is the first in its \n","      # corresponding matching span, assign 'B-\"match_id\"' as the label\n","      if index == df['ALL_INDICES'][match_id_index][0]:\n","        labels.append(\"B-\" + str(df['MATCH_IDS'][match_id_index]))\n","      # If match is found for a word, and its index is not the first in its \n","      # corresponding matching span, assign 'I-\"match_id\"' as the label\n","      else:\n","        labels.append(\"I-\" + str(df['MATCH_IDS'][match_id_index]))\n","    index += 1\n","  \n","  # Create new column that contains a list of labels for every word in \n","  # the sentence\n","  df['LABELS'] = labels\n","  return df\n","\n","  # Usage example: transformDF = matcherDF.apply(apply_labels,axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ewrDs31biMj","colab_type":"code","colab":{}},"source":["# shortens all sentences to the desired length\n","# sentence with 100 words because 1 sentences with 64 words and one with 36 words\n","# inputs: string/sentence and max length\n","# returns list of lists\n","def getShorterSent(s, n):\n","    pieces = s.split()\n","    return list(\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)) \n","\n","# shortens all sentences in a discharge summary\n","# inputs: discharge note as a string and max sentence length\n","# returns list of lists -- list of all discharge summary sentences with max length \"words\"\n","def preprocess(text, words):\n","    short_sent = list(map(lambda x: getShorterSent(x, words), text))\n","    final_list = list(itertools.chain(*short_sent))\n","    return final_list\n","\n","# removes unnecessary puctuation and extra spaces\n","# input string/sentence\n","# retruns string/sentence\n","def clean_sentence(x):\n","  # added to remove all punctuation because of spacy processing\n","  x_clean = x.translate(str.maketrans('', '', string.punctuation))\n","  x_clean = x_clean.strip()\n","  x_clean = re.sub(' +', ' ', x_clean)\n","  return x_clean\n","\n","# df should be the icustay_id and notes column from the original MIMIC dataset\n","# labeled ids and notes\n","# words = max sentence length\n","def getLabeledDF(df, phrase_matcher, words):\n","\n","  # preprocesses dataframe by shortening sentences to the maximum desired length\n","  sent_listed = pd.DataFrame({\n","    'ids': df.ids, \n","    'notes': df.notes.apply(lambda x: str(x).split('\\n')).apply(lambda x: preprocess(x, words))\n","    })\n","  \n","  # puts each sentence in its own row with corresponding icustay_id \n","  melted_df = sent_listed.notes.apply(pd.Series) \\\n","    .merge(sent_listed, right_index = True, left_index = True) \\\n","    .drop([\"notes\"], axis = 1) \\\n","    .melt(id_vars = ['ids'], value_name = \"notes\").sort_values(['ids', 'variable']) \\\n","    .dropna() \n","\n","  #free memory\n","  del sent_listed\n","\n","  #formate the melted dataframe for apply_matcher function\n","  melted_df = melted_df.rename(columns={'ids':'ID', 'notes':'SENTENCE'})\n","  melted_df.SENTENCE = melted_df.SENTENCE.apply(clean_sentence)\n","\n","  # apply matcher function\n","  ready_to_label = apply_matcher(melted_df, phrase_matcher)\n","\n","  del melted_df\n","  # using above output generate the list of the actual labels\n","  transformDF = ready_to_label.apply(apply_labels,axis=1) \n","  del ready_to_label\n","\n","  return(transformDF)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6milrYIX8aET","colab_type":"code","colab":{}},"source":["# split sentences into words\n","def splitSentence(x):\n","  return x.split(' ') \n","\n","# generate final csv for model training\n","# num_notes = number of discharge notes\n","# name is file name\n","# words is max sentence length\n","def generateCSV(num_notes, name, words):\n","  # file path\n","  file_name = './drive/My Drive/MLHC Final Project/Final Preprocessed Data/' + name\n","  # determines the number of notes in the final csv\n","  # -1 will indicate to use all of the discharge notes\n","  if num_notes == -1:\n","    notes_to_try = pd.DataFrame({\n","      'ids': discharge_df.ROW_ID, \n","      'notes': discharge_df.TEXT\n","    })\n","  # only uses a specified subset of 1000 notes\n","  else:\n","    notes_to_try = pd.DataFrame({\n","      'ids': discharge_df.ROW_ID[(1000*num_notes):(1000*(num_notes+1))], \n","      'notes': discharge_df.TEXT[(1000*num_notes):(1000*(num_notes+1))]\n","    })\n","  # generate spacy phrase matcher object\n","  pm = generatePhraseMatcher()\n","\n","  # generate dataframe\n","  df = getLabeledDF(notes_to_try, pm, words)\n","\n","  del notes_to_try\n","\n","  # split sentences into words\n","  df.SENTENCE = df.SENTENCE.apply(splitSentence)\n","\n","  # generates a dataframe with each word in its own row\n","  # overall_sent is unique\n","  # id is the original note ID\n","  # SENTENCE_ID is the sentence number within that note\n","  note_id = df[['ID', 'SENTENCE', 'LABELS']].set_index(['ID']).apply(pd.Series.explode).reset_index()\n","  sent_id = df[['variable', 'SENTENCE', 'LABELS']].set_index(['variable']).apply(pd.Series.explode).reset_index()\n","  final_df = pd.concat([sent_id['variable'], note_id], axis=1).rename(columns = {'ID':'NOTE_ID', 'SENTENCE':'WORD', 'variable':'SENTENCE_ID'})\n","  final_df['overall_sent'] = final_df.groupby(['NOTE_ID', 'SENTENCE_ID']).ngroup()\n","\n","  del df\n","  del note_id\n","  del sent_id\n","  # final_df.to_csv(file_name, header=True)\n","\n","  return(final_df) \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"umBPmBf5soBu","colab_type":"code","colab":{}},"source":["# create object to set a timer on runtime\n","import signal\n","class TimeoutException(Exception):   # Custom exception class\n","    pass\n","def timeout_handler(signum, frame):   # Custom signal handler\n","    raise TimeoutException\n","# Change the behavior of SIGALRM\n","signal.signal(signal.SIGALRM, timeout_handler)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3DNa7pcEQfj","colab_type":"code","colab":{}},"source":["# function to run generatCSV over groupd of 1000\n","def loopThrough(start, stop, num):\n","  temp = pd.DataFrame()\n","  for i in range(start, stop):\n","    signal.alarm(3600)\n","    try:\n","      if i == start:\n","        temp = generateCSV(i, '50_notes_discharge_umls_clean_length_126.csv', num)\n","      else: \n","        temp = pd.concat([temp, generateCSV(i, '50_notes_discharge_umls_clean_length_126.csv', num)])\n","    except TimeoutException:\n","        continue # continue the for loop if function A takes more than 5 second\n","    else:\n","        # Reset the alarm\n","        signal.alarm(0)\n","  return(temp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"giyMqS2IwASc","colab_type":"code","colab":{}},"source":["import time\n","# just to time how long it takes\n","start_time = time.time()\n","\n","# first number is your starting point -- the function reads sets of 1000 \n","# Example: 0, 2 --> 2000 notes --> 0:1000, 1000:2000\n","# final number is how long the max sentence length should be\n","# the function returns these dataframes concatenated together\n","df_for_modeling = loopThrough(1, 3, 126)\n","\n","# print out runtime\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8r3aQFScr_C8","colab_type":"code","colab":{}},"source":["# appends the column with unique sentence id's\n","df_for_modeling['overall_sent'] = df_for_modeling.groupby(['NOTE_ID', 'SENTENCE_ID']).ngroup()\n","# resets the index\n","df_for_modeling = df_for_modeling.reset_index()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvO0jKSVvwzd","colab_type":"code","colab":{}},"source":["# finally write the file to a csv\n","df_for_modeling.dropna().to_csv('./drive/My Drive/MLHC Final Project/Final Preprocessed Data/2000_notes_discharge_umls_clean_length_126.csv', header=True)"],"execution_count":0,"outputs":[]}]}