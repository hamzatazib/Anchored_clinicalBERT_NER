{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that this section of code is adapted from the clinicalBERT github repo: \n",
    "## https://github.com/EmilyAlsentzer/clinicalBERT/blob/169542dcdb6eda1fdbc6696e766818a3bb9601a4/downstream_tasks/run_ner.py#L157\n",
    "\n",
    "def read_data(input_file):\n",
    "    with open(input_file) as f:\n",
    "        lines = []\n",
    "        words = []\n",
    "        labels = []\n",
    "        all_words = []\n",
    "        all_labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0: #i.e. spaces indicate we are in between sentences\n",
    "                assert len(words) == len(labels)\n",
    "                if len(words) == 0:\n",
    "                    continue\n",
    "                lines.append([words, labels])\n",
    "                all_words.append(words)\n",
    "                all_labels.append(labels)\n",
    "                words = []\n",
    "                labels = []\n",
    "                continue\n",
    "\n",
    "            word = line.split()[0]\n",
    "            label = line.split()[-1]\n",
    "            words.append(word)\n",
    "            labels.append(label)\n",
    "\n",
    "        return all_words, all_labels, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from a list.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_list = word_list output from read_data\n",
    "# label_list = label_list output from read_data\n",
    "# n = maximum sentence length to be considered for splitting\n",
    "\n",
    "def process_data(sent_list, label_list, n):\n",
    "    print(\"Length of sentence list: \",len(sent_list))\n",
    "    print(\"Length of label list: \",len(label_list))\n",
    "    \n",
    "    # Chunk lists based on input parameter n\n",
    "    chunked_sent = []\n",
    "    for i in sent_list:\n",
    "        chunks = list(create_chunks(i,n))\n",
    "        chunked_sent.append(chunks)\n",
    "        \n",
    "    chunked_labels = []\n",
    "    for i in label_list:\n",
    "        chunks = list(create_chunks(i,n))\n",
    "        chunked_labels.append(chunks)\n",
    "    \n",
    "    # Format output as dataframes where each row contains a sentence or set of labels for that sentence\n",
    "    chunked_sent_df = pd.DataFrame(chunked_sent)\n",
    "    stacked_chunked_sent_df = pd.DataFrame(chunked_sent_df.stack())\n",
    "    stacked_chunked_sent_df.columns = ['SENTENCE']\n",
    "    stacked_chunked_sent_df['INDEX_COL'] = stacked_chunked_sent_df.index.get_level_values(0)\n",
    "    stacked_chunked_sent_df['SUB_INDEX_COL'] = stacked_chunked_sent_df.index.get_level_values(1)\n",
    "    stacked_chunked_sent_df['SENTENCE_LENGTH'] = stacked_chunked_sent_df['SENTENCE'].astype(str).str.split().str.len()\n",
    "    \n",
    "    chunked_labels_df = pd.DataFrame(chunked_labels)\n",
    "    stacked_chunked_labels_df = pd.DataFrame(chunked_labels_df.stack())\n",
    "    stacked_chunked_labels_df.columns = ['LABELS']\n",
    "    \n",
    "    # Join the sentence dataframe with the label dataframe, then reset index\n",
    "    merged_df = stacked_chunked_sent_df.join(stacked_chunked_labels_df)\n",
    "    re_indx_df = merged_df.reset_index(drop=True)\n",
    "    \n",
    "    # Unique ID will not act as the unique identifier for each chunked sentence\n",
    "    re_indx_df['UNIQUE_ID'] = re_indx_df.index\n",
    "    \n",
    "    # Reorder columns\n",
    "    re_indx_df = re_indx_df[['INDEX_COL',\n",
    "                             'SUB_INDEX_COL',\n",
    "                             'UNIQUE_ID',\n",
    "                             'SENTENCE_LENGTH',\n",
    "                             'SENTENCE',\n",
    "                             'LABELS']]\n",
    "    \n",
    "    # Check that all rows have maximum number of words equal to n\n",
    "    print(\"Maximum chunked sentence length: \", max(re_indx_df.SENTENCE_LENGTH))\n",
    "    \n",
    "    # Explode dataframe so that each row contains a word and its corresponding label\n",
    "    explode_df = re_indx_df.set_index(['UNIQUE_ID']).apply(pd.Series.explode).reset_index()\n",
    "    \n",
    "    return explode_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_train, label_list_train, lines_list_train = read_data(\"./processed/merged/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_train_df = process_data(word_list_train, label_list_train, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on sentence 187, 188\n",
    "reformat_train_df.loc[reformat_train_df['INDEX_COL'] == 187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "reformat_train_df.to_csv('./processed_data_126/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_dev, label_list_dev, lines_list_dev = read_data(\"./processed/merged/dev.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_dev_df = process_data(word_list_dev, label_list_dev, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "reformat_dev_df.to_csv('./processed_data_126/dev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_test, label_list_test, lines_list_test = read_data(\"./processed/merged/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_test_df = process_data(word_list_test, label_list_test, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "reformat_test_df.to_csv('./processed_data_126/test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
