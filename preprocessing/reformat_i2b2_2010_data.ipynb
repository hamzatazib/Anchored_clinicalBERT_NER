{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that this code is adapted from the clinicalBERT github repo: \n",
    "\n",
    "https://github.com/EmilyAlsentzer/clinicalBERT/tree/master/downstream_tasks/i2b2_preprocessing/i2b2_2010_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:50:53.281029Z",
     "start_time": "2019-10-03T15:50:53.185572Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, re, pickle, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:50:54.701759Z",
     "start_time": "2019-10-03T15:50:54.126088Z"
    }
   },
   "outputs": [],
   "source": [
    "# List number of beth concept files\n",
    "ls raw/concept_assertion_relation_training_data/beth/concept| wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:50:56.387400Z",
     "start_time": "2019-10-03T15:50:55.960811Z"
    }
   },
   "outputs": [],
   "source": [
    "# List number of partners concept files\n",
    "ls raw/concept_assertion_relation_training_data/partners/concept| wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:50:58.037429Z",
     "start_time": "2019-10-03T15:50:57.602927Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls raw/concept_assertion_relation_training_data/beth/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:50:59.407222Z",
     "start_time": "2019-10-03T15:50:59.402358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set directories for training and testing data\n",
    "TRAIN_DIRS = [\n",
    "    './raw/concept_assertion_relation_training_data/beth/',\n",
    "    './raw/concept_assertion_relation_training_data/partners/',\n",
    "]\n",
    "TEST_DIR = './raw/reference_standard_for_test_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:51:04.470172Z",
     "start_time": "2019-10-03T15:51:04.417417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process file\n",
    "def process_concept(concept_str):\n",
    "    \"\"\"\n",
    "    takes string like\n",
    "    'c=\"asymptomatic\" 16:2 16:2||t=\"problem\"'\n",
    "    and returns dictionary like\n",
    "    {'t': 'problem', 'start_line': 16, 'start_pos': 2, 'end_line': 16, 'end_pos': 2}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        position_bit, problem_bit = concept_str.split('||')\n",
    "        t = problem_bit[3:-1]\n",
    "        \n",
    "        start_and_end_span = next(re.finditer('\\s\\d+:\\d+\\s\\d+:\\d+', concept_str)).span()\n",
    "        c = concept_str[3:start_and_end_span[0]-1]\n",
    "        c = [y for y in c.split(' ') if y.strip() != '']\n",
    "        c = ' '.join(c)\n",
    "\n",
    "        start_and_end = concept_str[start_and_end_span[0]+1 : start_and_end_span[1]]\n",
    "        start, end = start_and_end.split(' ')\n",
    "        start_line, start_pos = [int(x) for x in start.split(':')]\n",
    "        end_line, end_pos = [int(x) for x in end.split(':')]\n",
    "        \n",
    "    except:\n",
    "        print(concept_str)\n",
    "        raise\n",
    "    \n",
    "    return {\n",
    "        't': t, 'start_line': start_line, 'start_pos': start_pos, 'end_line': end_line, 'end_pos': end_pos,\n",
    "        'c': c, \n",
    "    }\n",
    "\n",
    "def build_label_vocab(base_dirs):\n",
    "    seen, label_vocab, label_vocab_size = set(['O']), {'O': 'O'}, 0\n",
    "    \n",
    "    for base_dir in base_dirs:\n",
    "        concept_dir = os.path.join(base_dir, 'concept')\n",
    "\n",
    "        assert os.path.isdir(concept_dir), \"Directory structure doesn't match!\"\n",
    "\n",
    "        ids = set([x[:-4] for x in os.listdir(concept_dir) if x.endswith('.con')])\n",
    "\n",
    "        for i in ids:\n",
    "            with open(os.path.join(concept_dir, '%s.con' % i)) as f:\n",
    "                concepts = [process_concept(x.strip()) for x in f.readlines()]\n",
    "            for c in concepts:\n",
    "                if c['t'] not in seen:\n",
    "                    label_vocab_size += 1\n",
    "                    label_vocab['B-%s' % c['t']] = 'B-%s' % c['t'] # label_vocab_size\n",
    "                    label_vocab_size += 1\n",
    "                    label_vocab['I-%s' % c['t']] = 'I-%s' % c['t'] # label_vocab_size\n",
    "                    seen.update([c['t']])\n",
    "    return label_vocab, label_vocab_size\n",
    "\n",
    "def reformatter(base, label_vocab, txt_dir = None, concept_dir = None):\n",
    "    if txt_dir is None: txt_dir = os.path.join(base, 'txt')\n",
    "    if concept_dir is None: concept_dir = os.path.join(base, 'concept')\n",
    "    \n",
    "    assert os.path.isdir(txt_dir) and os.path.isdir(concept_dir), \"Directory structure doesn't match!\"\n",
    "    \n",
    "    txt_ids = set([x[:-4] for x in os.listdir(txt_dir) if x.endswith('.txt')])\n",
    "    concept_ids = set([x[:-4] for x in os.listdir(concept_dir) if x.endswith('.con')])\n",
    "    \n",
    "    assert txt_ids == concept_ids, (\n",
    "        \"id set doesn't match: txt - concept = %s, concept - txt = %s\"\n",
    "        \"\" % (str(txt_ids - concept_ids), str(concept_ids - txt_ids))\n",
    "    )\n",
    "    \n",
    "    ids = txt_ids\n",
    "    \n",
    "    reprocessed_texts = {}\n",
    "    for i in ids:\n",
    "        with open(os.path.join(txt_dir, '%s.txt' % i), mode='r') as f:\n",
    "            lines = f.readlines()\n",
    "            txt = [[y for y in x.strip().split(' ') if y.strip() != ''] for x in lines]\n",
    "            line_starts_with_space = [x.startswith(' ') for x in lines]\n",
    "        with open(os.path.join(concept_dir, '%s.con' % i), mode='r') as f:\n",
    "            concepts = [process_concept(x.strip()) for x in f.readlines()]\n",
    "            \n",
    "        labels = [['O' for _ in line] for line in txt]\n",
    "        for c in concepts:\n",
    "            if c['start_line'] == c['end_line']:\n",
    "                line = c['start_line']-1\n",
    "                p_modifier = -1 if line_starts_with_space[line] else 0\n",
    "                text = (' '.join(txt[line][c['start_pos']+p_modifier:c['end_pos']+1+p_modifier])).lower()\n",
    "                assert text == c['c'], (\n",
    "                    \"Text mismatch! %s vs. %s (id: %s, line: %d)\\nFull line: %s\"\n",
    "                    \"\" % (c['c'], text, i, line, txt[line])\n",
    "                )\n",
    "                \n",
    "            for line in range(c['start_line']-1, c['end_line']):\n",
    "                p_modifier = -1 if line_starts_with_space[line] else 0\n",
    "                start_pos = c['start_pos']+p_modifier if line == c['start_line']-1 else 0\n",
    "                end_pos   = c['end_pos']+1+p_modifier if line == c['end_line']-1 else len(txt[line])\n",
    "                \n",
    "                if line == c['end_line'] - 1: labels[line][end_pos-1] = label_vocab['I-%s' % c['t']]                \n",
    "                if line == c['start_line'] - 1: labels[line][start_pos] = label_vocab['B-%s' % c['t']]\n",
    "                for j in range(start_pos + 1, end_pos-1): labels[line][j] = label_vocab['I-%s' % c['t']]\n",
    "            \n",
    "        joined_words_and_labels = [zip(txt_line, label_line) for txt_line, label_line in zip(txt, labels)]\n",
    "\n",
    "        out_str = '\\n\\n'.join(\n",
    "            ['\\n'.join(['%s %s' % p for p in joined_line]) for joined_line in joined_words_and_labels]\n",
    "        )\n",
    "        \n",
    "        reprocessed_texts[i] = out_str\n",
    "        \n",
    "    return reprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:51:07.369475Z",
     "start_time": "2019-10-03T15:51:06.938175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build label vocab\n",
    "label_vocab, label_vocab_size = build_label_vocab([\n",
    "    'raw/concept_assertion_relation_training_data/beth/',\n",
    "    'raw/concept_assertion_relation_training_data/partners/',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:51:09.031392Z",
     "start_time": "2019-10-03T15:51:09.009718Z"
    }
   },
   "outputs": [],
   "source": [
    "label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:51:22.458917Z",
     "start_time": "2019-10-03T15:51:20.939484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the individual text files\n",
    "reprocessed_texts = {\n",
    "    'beth':     reformatter('raw/concept_assertion_relation_training_data/beth/', label_vocab),\n",
    "    'partners': reformatter('raw/concept_assertion_relation_training_data/partners/', label_vocab),\n",
    "    'test':     reformatter(\n",
    "        'raw/reference_standard_for_test_data/', label_vocab,\n",
    "        txt_dir='raw/test_data/',\n",
    "        concept_dir='raw/reference_standard_for_test_data/concepts'\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T15:51:26.385606Z",
     "start_time": "2019-10-03T15:51:26.378774Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, txt_by_record in reprocessed_texts.items(): print(\"%s: %d\" % (key, len(txt_by_record)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example\n",
    "print(reprocessed_texts['beth']['record-37'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training into training and validation\n",
    "all_partners_train_ids = np.random.permutation(list(reprocessed_texts['partners'].keys()))\n",
    "N = len(all_partners_train_ids)\n",
    "N_train = int(0.9 * N)\n",
    "\n",
    "partners_train_ids = all_partners_train_ids[:N_train]\n",
    "partners_dev_ids = all_partners_train_ids[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Partners # Patients: Train: %d, Dev: %d\" %(len(partners_train_ids), len(partners_dev_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training into training and validation\n",
    "all_beth_train_ids = np.random.permutation(list(reprocessed_texts['beth'].keys()))\n",
    "N = len(all_beth_train_ids)\n",
    "N_train = int(0.9 * N)\n",
    "\n",
    "beth_train_ids = all_beth_train_ids[:N_train]\n",
    "beth_dev_ids = all_beth_train_ids[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beth # Patients: Train: %d, Dev: %d\" % (len(beth_train_ids), len(beth_dev_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print size of merged training and validation sets\n",
    "print(\"Merged # Patients: Train: %d, Dev: %d\" % (\n",
    "  len(partners_train_ids) + len(beth_train_ids), len(beth_dev_ids) + len(partners_dev_ids)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join files for training, validation, and test\n",
    "merged_train_txt = '\\n\\n'.join(np.random.permutation(\n",
    "    [reprocessed_texts['partners'][i] for i in partners_train_ids] + \n",
    "    [reprocessed_texts['beth'][i] for i in beth_train_ids]\n",
    "))\n",
    "merged_dev_txt = '\\n\\n'.join(np.random.permutation(\n",
    "    [reprocessed_texts['partners'][i] for i in partners_dev_ids] + \n",
    "    [reprocessed_texts['beth'][i] for i in beth_dev_ids]\n",
    "))\n",
    "merged_test_txt = '\\n\\n'.join(np.random.permutation(list(reprocessed_texts['test'].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merged # Samples: Train: %d, Dev: %d, Test: %d\" % (\n",
    "    len(merged_train_txt.split('\\n\\n')),\n",
    "    len(merged_dev_txt.split('\\n\\n')),\n",
    "    len(merged_test_txt.split('\\n\\n'))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partners_train_txt = '\\n\\n'.join(np.random.permutation(\n",
    "    [reprocessed_texts['partners'][i] for i in partners_train_ids]\n",
    "))\n",
    "partners_dev_txt = '\\n\\n'.join(np.random.permutation(\n",
    "    [reprocessed_texts['partners'][i] for i in partners_dev_ids]\n",
    "))\n",
    "partners_test_txt = '\\n\\n'.join(np.random.permutation(list(reprocessed_texts['test'].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FILES = {\n",
    "    'merged_train': './processed/merged/train.tsv',\n",
    "    'merged_dev':   './processed/merged/dev.tsv',\n",
    "    'merged_test':  './processed/merged/test.tsv', \n",
    "    'partners_train': './processed/partners/train.tsv',\n",
    "    'partners_dev':   './processed/partners/dev.tsv',\n",
    "    'partners_test':  './processed/partners/test.tsv', \n",
    "    'vocab': './processed/label_vocab.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files\n",
    "with open(OUT_FILES['merged_train'], mode='w') as f: f.write(merged_train_txt)\n",
    "with open(OUT_FILES['merged_dev'], mode='w') as f: f.write(merged_dev_txt)\n",
    "with open(OUT_FILES['merged_test'], mode='w') as f: f.write(merged_test_txt)\n",
    "with open(OUT_FILES['partners_train'], mode='w') as f: f.write(partners_train_txt)\n",
    "with open(OUT_FILES['partners_dev'], mode='w') as f: f.write(partners_dev_txt)\n",
    "with open(OUT_FILES['partners_test'], mode='w') as f: f.write(partners_test_txt)\n",
    "with open(OUT_FILES['vocab'], mode='wb') as f: pickle.dump(label_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
