{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that this code is adapted from the clinicalBERT github repo: \n",
    "\n",
    "https://github.com/EmilyAlsentzer/clinicalBERT/blob/169542dcdb6eda1fdbc6696e766818a3bb9601a4/lm_pretraining/format_mimic_for_BERT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import stanfordnlp\n",
    "import time\n",
    "import scispacy\n",
    "from tqdm import tqdm\n",
    "from heuristic_tokenize import sent_tokenize_rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting sentence boundaries\n",
    "def sbd_component(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # define sentence start if period + titlecase token\n",
    "        if token.text == '.' and doc[i+1].is_title:\n",
    "            doc[i+1].sent_start = True\n",
    "        if token.text == '-' and doc[i+1].text != '-':\n",
    "            doc[i+1].sent_start = True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting de-identification text into one token\n",
    "def fix_deid_tokens(text, processed_text):\n",
    "    deid_regex  = r\"\\[\\*\\*.{0,15}.*?\\*\\*\\]\" \n",
    "    if text:\n",
    "        indexes = [m.span() for m in re.finditer(deid_regex,text,flags=re.IGNORECASE)]\n",
    "    else:\n",
    "        indexes = []\n",
    "    for start,end in indexes:\n",
    "        processed_text.merge(start_idx=start,end_idx=end)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_section(section, note, processed_sections):\n",
    "    # perform spacy processing on section\n",
    "    processed_section = nlp(section['sections'])\n",
    "    processed_section = fix_deid_tokens(section['sections'], processed_section)\n",
    "    processed_sections.append(processed_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_note_helper(note):\n",
    "    # split note into sections\n",
    "    note_sections = sent_tokenize_rules(note)\n",
    "    processed_sections = []\n",
    "    section_frame = pd.DataFrame({'sections':note_sections})\n",
    "    section_frame.apply(process_section, args=(note,processed_sections,), axis=1)\n",
    "    return(processed_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(sent, note):\n",
    "    sent_text = sent['sents'].text\n",
    "    if len(sent_text) > 0 and sent_text.strip() != '\\n':\n",
    "        if '\\n' in sent_text:\n",
    "            sent_text = sent_text.replace('\\n', ' ')\n",
    "        note['TEXT'] += sent_text + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(processed_section, note):\n",
    "    # get sentences from spacy processing\n",
    "    sent_frame = pd.DataFrame({'sents': list(processed_section['sections'].sents)})\n",
    "    sent_frame.apply(process_text, args=(note,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_note(note):\n",
    "    try:\n",
    "        note_text = note['TEXT'] #unicode(note['text'])\n",
    "        note['TEXT'] = ''\n",
    "        processed_sections = process_note_helper(note_text)\n",
    "        ps = {'sections': processed_sections}\n",
    "        ps = pd.DataFrame(ps)\n",
    "        ps.apply(get_sentences, args=(note,), axis=1)\n",
    "        return note\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print ('error', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(sys.argv) < 2:\n",
    "        print('Please specify the note category.')\n",
    "        sys.exit()\n",
    "\n",
    "category = 'Discharge summary'\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "tqdm.pandas()\n",
    "\n",
    "print('Begin reading notes')\n",
    "\n",
    "\n",
    "# Uncomment this to use postgres to query mimic instead of reading from a file\n",
    "# con = psycopg2.connect(dbname='mimic', host=\"/var/run/postgresql\")\n",
    "# notes_query = \"(select * from mimiciii.noteevents);\"\n",
    "# notes = pd.read_sql_query(notes_query, con)\n",
    "notes = pd.read_csv('MIMIC_notes_discharge_sum.csv', index_col = 0)\n",
    "notes = notes[0:10000]\n",
    "print(set(notes['CATEGORY'])) # all categories\n",
    "\n",
    "\n",
    "notes = notes[notes['CATEGORY'] == category]\n",
    "print('Number of notes: %d' %len(notes.index))\n",
    "notes['ind'] = list(range(len(notes.index)))\n",
    "\n",
    "# NOTE: `disable=['tagger', 'ner'] was added after paper submission to make this process go faster\n",
    "# our time estimate in the paper did not include the code to skip spacy's NER & tagger\n",
    "nlp = spacy.load('en_core_sci_md', disable=['tagger','ner'])\n",
    "nlp.add_pipe(sbd_component, before='parser')  \n",
    "\n",
    "\n",
    "formatted_notes = notes.progress_apply(process_note, axis=1)\n",
    "# with open('/processed_mimic'  + category + '.txt','w') as f:\n",
    "#     for text in formatted_notes['TEXT']:\n",
    "#         if text != None and len(text) != 0 :\n",
    "#             f.write(text)\n",
    "#             f.write('\\n')\n",
    "\n",
    "end = time.time()\n",
    "print (end-start)\n",
    "print (\"Done formatting notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "formatted_notes.to_csv('./processed_mimic/first_10000_processed_mimic_notes_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
